{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMx4nlx4KRgBDFnyrgJggrY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdelrahman-toni/Classification/blob/main/classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S823A0_SfA0H"
      },
      "outputs": [],
      "source": [
        "!pip install pyproj\n",
        "!pip install missingno"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "accidents = pd.read_csv('/content/drive/MyDrive/data_eng/1995_Accidents_UK.csv',index_col='accident_reference')\n"
      ],
      "metadata": {
        "id": "a_6VjWLAfsiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest Neighbors (KNN)\n",
        "\n"
      ],
      "metadata": {
        "id": "OUy2sGhNf40X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Initialize the KNN classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Train the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
        "class_report_knn = classification_report(y_test, y_pred_knn)\n",
        "\n",
        "print(\"KNN Accuracy:\", accuracy_knn)\n",
        "print(\"KNN Confusion Matrix:\\n\", conf_matrix_knn)\n",
        "print(\"KNN Classification Report:\\n\", class_report_knn)\n"
      ],
      "metadata": {
        "id": "kdPJg_awf9HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes\n"
      ],
      "metadata": {
        "id": "B-b71VE1f-g5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Initialize the Naive Bayes classifier\n",
        "nb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_nb = nb.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)\n",
        "class_report_nb = classification_report(y_test, y_pred_nb)\n",
        "\n",
        "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
        "print(\"Naive Bayes Confusion Matrix:\\n\", conf_matrix_nb)\n",
        "print(\"Naive Bayes Classification Report:\\n\", class_report_nb)\n"
      ],
      "metadata": {
        "id": "kJcJz_RUgAX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression\n"
      ],
      "metadata": {
        "id": "vdtNrWcegCN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the Logistic Regression classifier\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "conf_matrix_lr = confusion_matrix(y_test, y_pred_lr)\n",
        "class_report_lr = classification_report(y_test, y_pred_lr)\n",
        "\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_lr)\n",
        "print(\"Logistic Regression Confusion Matrix:\\n\", conf_matrix_lr)\n",
        "print(\"Logistic Regression Classification Report:\\n\", class_report_lr)\n"
      ],
      "metadata": {
        "id": "35OBjCdLgDqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees\n"
      ],
      "metadata": {
        "id": "dqcJPanqgHNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize the Decision Tree classifier\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# Train the model\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "conf_matrix_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "class_report_dt = classification_report(y_test, y_pred_dt)\n",
        "\n",
        "print(\"Decision Trees Accuracy:\", accuracy_dt)\n",
        "print(\"Decision Trees Confusion Matrix:\\n\", conf_matrix_dt)\n",
        "print(\"Decision Trees Classification Report:\\n\", class_report_dt)\n"
      ],
      "metadata": {
        "id": "tYQdgsELgIAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forests\n"
      ],
      "metadata": {
        "id": "uwdULn2pgJqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "class_report_rf = classification_report(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Random Forests Accuracy:\", accuracy_rf)\n",
        "print(\"Random Forests Confusion Matrix:\\n\", conf_matrix_rf)\n",
        "print(\"Random Forests Classification Report:\\n\", class_report_rf)\n"
      ],
      "metadata": {
        "id": "1VN51HUZgLFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregate Results\n"
      ],
      "metadata": {
        "id": "1MNbfchXgOxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store the results\n",
        "results = {\n",
        "    'Model': ['KNN', 'Naive Bayes', 'Logistic Regression', 'Decision Trees', 'Random Forests'],\n",
        "    'Accuracy': [accuracy_knn, accuracy_nb, accuracy_lr, accuracy_dt, accuracy_rf],\n",
        "    'Precision': [\n",
        "        class_report_knn.split()[-9], class_report_nb.split()[-9],\n",
        "        class_report_lr.split()[-9], class_report_dt.split()[-9],\n",
        "        class_report_rf.split()[-9]\n",
        "    ],\n",
        "    'Recall': [\n",
        "        class_report_knn.split()[-8], class_report_nb.split()[-8],\n",
        "        class_report_lr.split()[-8], class_report_dt.split()[-8],\n",
        "        class_report_rf.split()[-8]\n",
        "    ],\n",
        "    'F1-Score': [\n",
        "        class_report_knn.split()[-7], class_report_nb.split()[-7],\n",
        "        class_report_lr.split()[-7], class_report_dt.split()[-7],\n",
        "        class_report_rf.split()[-7]\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert the results dictionary to a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "WrT6DG31gQGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Performance\n"
      ],
      "metadata": {
        "id": "xhz9YzqmgRpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(results_df['Model'], results_df['Accuracy'], color='skyblue')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.show()\n",
        "\n",
        "# Plotting precision, recall, and F1-score\n",
        "metrics = ['Precision', 'Recall', 'F1-Score']\n",
        "for metric in metrics:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(results_df['Model'], results_df[metric], color='skyblue')\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel(metric)\n",
        "    plt.title(f'Model {metric} Comparison')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "gGbAvppNgSI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Performance Comparison\n",
        "Aggregate Results"
      ],
      "metadata": {
        "id": "BsjlwoLQgTI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store the results\n",
        "results = {\n",
        "    'Model': ['KNN', 'Naive Bayes', 'Logistic Regression', 'Decision Trees', 'Random Forests'],\n",
        "    'Accuracy': [accuracy_knn, accuracy_nb, accuracy_lr, accuracy_dt, accuracy_rf],\n",
        "    'Precision': [\n",
        "        precision_knn, precision_nb, precision_lr, precision_dt, precision_rf\n",
        "    ],\n",
        "    'Recall': [\n",
        "        recall_knn, recall_nb, recall_lr, recall_dt, recall_rf\n",
        "    ],\n",
        "    'F1-Score': [\n",
        "        f1_knn, f1_nb, f1_lr, f1_dt, f1_rf\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert the results dictionary to a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "z-1QPRqhgWQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Performance\n"
      ],
      "metadata": {
        "id": "DfQWz_cogYCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(results_df['Model'], results_df['Accuracy'], color='skyblue')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.show()\n",
        "\n",
        "# Plotting precision, recall, and F1-score\n",
        "metrics = ['Precision', 'Recall', 'F1-Score']\n",
        "for metric in metrics:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(results_df['Model'], results_df[metric], color='skyblue')\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel(metric)\n",
        "    plt.title(f'Model {metric} Comparison')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Vw0kxGcngYpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final"
      ],
      "metadata": {
        "id": "AyW0cp3LgeWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {\n",
        "    'Model': ['KNN', 'Naive Bayes', 'Logistic Regression', 'Decision Trees', 'Random Forests'],\n",
        "    'Accuracy': [accuracy_knn, accuracy_nb, accuracy_lr, accuracy_dt, accuracy_rf],\n",
        "    'Precision': [\n",
        "        class_report_knn.split()[-9], class_report_nb.split()[-9],\n",
        "        class_report_lr.split()[-9], class_report_dt.split()[-9],\n",
        "        class_report_rf.split()[-9]\n",
        "    ],\n",
        "    'Recall': [\n",
        "        class_report_knn.split()[-8], class_report_nb.split()[-8],\n",
        "        class_report_lr.split()[-8], class_report_dt.split()[-8],\n",
        "        class_report_rf.split()[-8]\n",
        "    ],\n",
        "    'F1-Score': [\n",
        "        class_report_knn.split()[-7], class_report_nb.split()[-7],\n",
        "        class_report_lr.split()[-7], class_report_dt.split()[-7],\n",
        "        class_report_rf.split()[-7]\n",
        "    ]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n",
        "\n",
        "# Visualize Performance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(results_df['Model'], results_df['Accuracy'], color='skyblue')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.show()\n",
        "\n",
        "# Precision, Recall, F1-Score\n",
        "metrics = ['Precision', 'Recall', 'F1-Score']\n",
        "for metric in metrics:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(results_df['Model'], results_df[metric], color='skyblue')\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel(metric)\n",
        "    plt.title(f'Model {metric} Comparison')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xuGsDB0-gfeb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}